import Head from 'next/head'
import Image from 'next/image'
import { Inter, Noto_Color_Emoji, Shadows_Into_Light_Two } from 'next/font/google'
import styles from '@/styles/Home.module.css'
import { useState, useEffect } from 'react'



import SpeechRecognition, { useSpeechRecognition } from 'react-speech-recognition';
// const Dictaphone = () => {
//   const {
//     transcript,
//     listening,
//     resetTranscript,
//     browserSupportsSpeechRecognition
//   } = useSpeechRecognition();

//   if (!browserSupportsSpeechRecognition) {
//     return <span>Browser doesn't support speech recognition.</span>;
//   }

//   return (
//     <div>
//       <p>Microphone: {listening ? 'on' : 'off'}</p>
//       <button onClick={SpeechRecognition.startListening}>Start</button>
//       <button onClick={SpeechRecognition.stopListening}>Stop</button>
//       <button onClick={resetTranscript}>Reset</button>
//       <p>{transcript}</p>
//     </div>
//   );
  
// };


const inter = Inter({ subsets: ['latin'] })



export default function Home() {

  const [isPaused, setIsPaused] = useState(false);
  // const [utterance, setUtterance] = useState(null);
  const [message, setMessage] = useState(""); 
  const [text, setText] = useState(""); 



  const commands = [
    {
      command: "over",
      callback: (dependency) => handlePlay()
    }
  ]

  const {
    transcript,
    listening,
    resetTranscript,
    browserSupportsSpeechRecognition
  } = useSpeechRecognition({ commands });

  const listenStart = () => {
    SpeechRecognition.startListening({continuous:true});
  }

  // text to speach code begin


  useEffect(() => {
    const synth = window.speechSynthesis;
    const u = new SpeechSynthesisUtterance(text);
    console.log("UseEffect Log: " + text);

    // setUtterance(u);

    u.addEventListener("end", (event) => {
      console.log("utterance has ended?");
      resetTranscript(); 
  
    }) 

    synth.speak(u); 

    return () => {
      synth.cancel();
    };
  }, [text]);

  const handlePlay = async () => {

    console.log(transcript);

    var requestBody = "data="+transcript //TODO: url encode transcript

    const response = await fetch("https://flask-hello-world-ruby-three.vercel.app/GPT_output", {
      method: "POST",
      body: requestBody,
      headers: {
        "Content-Type": "application/x-www-form-urlencoded"
      }
    })

    var data = await response.text()

    // text = data;
    setText(data); 
    // console.log(text);
    // console.log(response.text);
    // console.log(response);
    setMessage(data); 



    // const synth = window.speechSynthesis;
    // console.log(utterance);
    // synth.speak(utterance);

  };


  const handlePause = () => {
    const synth = window.speechSynthesis;
    synth.pause();
    setIsPaused(true);
  };

  const handleStop = () => {
    const synth = window.speechSynthesis;
    synth.cancel();
    setIsPaused(false);
  };
// text to speach code end

  return (
    <>
      <Head>
        <title>Create Next App</title>
        <meta name="description" content="Generated by create next app" />
        <meta name="viewport" content="width=device-width, initial-scale=1" />
        <link rel="icon" href="/favicon.ico" />
      </Head>
      <main className={`${styles.main} ${inter.className}`}>
        <div className={styles.description}>
          <p>Microphone: {listening ? 'on' : 'off'}</p>
          <button onClick={listenStart}>Start</button>
          <button onClick={SpeechRecognition.stopListening}>Stop</button>
          <button onClick={resetTranscript}>Reset</button>
          <button onClick={handlePlay}>Play Speech</button> 
          <p>{message}</p>
          <p>{transcript}</p>
        </div>

        <div className={styles.center}>
          <Image
            className={styles.logo}
            src="/next.svg"
            alt="Next.js Logo"
            width={180}
            height={37}
            priority
          />
        </div>

      </main>
    </>
  )
}
